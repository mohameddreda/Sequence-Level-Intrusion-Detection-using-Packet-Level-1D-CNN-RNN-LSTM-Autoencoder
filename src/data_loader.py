# src/data_loader.py
import pandas as pd
import numpy as np
import os
from config import Config

class DataLoader:
    def __init__(self):
        self.config = Config()
    
    def load_data(self):
        """Load data from all available sources"""
        try:
            # Option 1: Ø§Ø³ØªØ®Ø¯Ø§Ù… training-set Ùˆ testing-set Ø¥Ø°Ø§ Ù…ÙˆØ¬ÙˆØ¯ÙŠÙ†
            if self.config.USE_SPLIT_FILES and os.path.exists(self.config.TRAIN_PATH):
                print("ğŸ“Š Loading split datasets...")
                train_df = pd.read_csv(self.config.TRAIN_PATH)
                test_df = pd.read_csv(self.config.TEST_PATH)
                df = pd.concat([train_df, test_df], ignore_index=True)
                print(f"âœ… Training set: {train_df.shape}")
                print(f"âœ… Testing set: {test_df.shape}")
            
            # Option 2: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ø±Ø¨Ø¹Ø©
            else:
                print("ğŸ“Š Loading from 4 CSV files...")
                dfs = []
                total_samples = 0

                # Try to read header names from the TRAIN_PATH if available
                header_names = None
                try:
                    if os.path.exists(self.config.TRAIN_PATH):
                        header_names = pd.read_csv(self.config.TRAIN_PATH, nrows=0).columns.tolist()
                except Exception:
                    header_names = None

                for path in self.config.DATA_PATHS:
                    if os.path.exists(path):
                        print(f"Loading {path}...")
                        # Many UNSW NB15 split files don't include headers â€” read without header
                        # and apply header names from the training-set if available.
                        if header_names:
                            df_part = pd.read_csv(path, nrows=20000, header=None, low_memory=False)
                            # If number of columns differs, only assign what's available
                            if df_part.shape[1] == len(header_names):
                                df_part.columns = header_names
                            else:
                                # If columns differ, try to set as many names as possible
                                n = min(df_part.shape[1], len(header_names))
                                df_part.columns = header_names[:n] + [f"col_{i}" for i in range(n, df_part.shape[1])]
                        else:
                            df_part = pd.read_csv(path, nrows=20000, header=None, low_memory=False)

                        dfs.append(df_part)
                        total_samples += len(df_part)
                        print(f"âœ… Loaded {len(df_part)} samples from {path}")
                    else:
                        print(f"âš ï¸ File not found: {path}")

                if not dfs:
                    print("âŒ No data files found!")
                    return None

                df = pd.concat(dfs, ignore_index=True)
                print(f"ğŸ‰ Combined dataset: {df.shape} (from {len(dfs)} files)")
            
            # Ù†ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
            self._validate_columns(df)
            
            return df
            
        except Exception as e:
            print(f"âŒ Error loading data: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _validate_columns(self, df):
        """Validate that required columns exist"""
        print("ğŸ” Validating columns...")
        
        # Ù†ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        required_columns = [self.config.LABEL_COLUMN, self.config.ATTACK_CAT_COLUMN]
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            print(f"âš ï¸ Missing columns: {missing_columns}")
            print(f"ğŸ“‹ Available columns: {list(df.columns)}")
        
        # Ù†ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù€ features Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©
        available_features = [f for f in self.config.SELECTED_FEATURES if f in df.columns]
        missing_features = [f for f in self.config.SELECTED_FEATURES if f not in df.columns]
        
        if missing_features:
            print(f"âš ï¸ Missing features: {missing_features}")
        
        print(f"ğŸ¯ Using {len(available_features)} available features out of {len(self.config.SELECTED_FEATURES)}")
    
    def get_basic_info(self, df):
        """Get basic dataset information"""
        if df is None:
            return {}
            
        info = {
            'shape': df.shape,
            'label_distribution': df[self.config.LABEL_COLUMN].value_counts() if self.config.LABEL_COLUMN in df.columns else "N/A",
            'attack_distribution': df[self.config.ATTACK_CAT_COLUMN].value_counts() if self.config.ATTACK_CAT_COLUMN in df.columns else "N/A"
        }
        
        return info